{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023/2/27\n",
    "* rewrite tokenizer by hugging face\n",
    "* rewrite dataloader via yield and add key_padding_mask\n",
    "* Tying weight between embedding and pre_softmax\n",
    "* rewriting Transformer model via TransformerLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* add Label Smooth\n",
    "* rewrite train() and evaluate    almost cause by BatchLoader and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023/2/28\n",
    "* rewrite BatchLoader make the total batch_tokens <= max_len\n",
    "* merge valid_loader and train_loader to one function by argument dataset\n",
    "* writer translate function for test\n",
    "* carry BatchLoader in dataLoader of torch (by batch_size=1)\n",
    "* change de -> en to de<->en(intertranslation)\n",
    "* limited the max_len of output <= input length + 50\n",
    "* in evaluate function,delete the tokens following \\<eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future\n",
    "### Test module\n",
    "* beam search\n",
    "\n",
    "\n",
    "### Train module\n",
    "* use colossal-ai to train model (data parallel and gradient accumulation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/zrs/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed_value = 721\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value) \n",
    "torch.manual_seed(seed_value)    \n",
    "torch.cuda.manual_seed(seed_value)      \n",
    "# torch.cuda.manual_seed_all(seed_value)   \n",
    "\n",
    "torch.backends.cudnn.benchmark = False        # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_tokens = 37000\n",
    "bos_id = 0\n",
    "eos_id = 1\n",
    "pad_id = 2\n",
    "\n",
    "# below two hypeparameter is not need\n",
    "# seq_len = 512\n",
    "# batch_size = 8\n",
    "\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dff = 2048\n",
    "N = 6 # num of encoder/decoder layers\n",
    "p_drop = 0.1\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "gpu_num = 1\n",
    "warmup_steps = 4000*8//gpu_num\n",
    "\n",
    "used_cuda = \"cuda:2\"\n",
    "device = torch.device(used_cuda if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "save_path = \"checkpoint.tar\"\n",
    "\n",
    "# other parameter in train() and spm.SentencePieceTrainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-17T12:09:59.376655Z",
     "iopub.status.busy": "2023-02-17T12:09:59.374968Z",
     "iopub.status.idle": "2023-02-17T12:25:08.578162Z",
     "shell.execute_reply": "2023-02-17T12:25:08.576340Z",
     "shell.execute_reply.started": "2023-02-17T12:09:59.376508Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"wmt14\", 'de-en', split='train')\n",
    "\n",
    "# with open(\"en.txt\",'w') as f:\n",
    "#     for i in range(len(dataset)):\n",
    "#         f.write(dataset[i]['translation']['en']+'\\n')\n",
    "        \n",
    "# with open(\"de.txt\",'w') as f:\n",
    "#     for i in range(len(dataset)):\n",
    "#         f.write(dataset[i]['translation']['de']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:25:08.581937Z",
     "iopub.status.busy": "2023-02-17T12:25:08.581441Z",
     "iopub.status.idle": "2023-02-17T12:25:08.588636Z",
     "shell.execute_reply": "2023-02-17T12:25:08.587034Z",
     "shell.execute_reply.started": "2023-02-17T12:25:08.581891Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncate the long sentence (though we can train transformer by any length ,the gpu memory cannot allow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:25:08.590772Z",
     "iopub.status.busy": "2023-02-17T12:25:08.590310Z",
     "iopub.status.idle": "2023-02-17T12:25:08.603615Z",
     "shell.execute_reply": "2023-02-17T12:25:08.602749Z",
     "shell.execute_reply.started": "2023-02-17T12:25:08.590725Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('dataset')\n",
    "\n",
    "de_en_pairs = []\n",
    "for i in range(len(dataset)):        \n",
    "    de_en_pairs.append((dataset[i]['translation']['de'][:512],dataset[i]['translation']['en'][:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_pairs = sorted(de_en_pairs,key=lambda x:+len(x[0])+len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(de_en_pairs[-1][0])+len(de_en_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(de_en_pairs[-500][0])+len(de_en_pairs[-500][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_en_pairs = de_en_pairs[:-500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_en_pairs[-1][0])+len(de_en_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(de_en_pairs[0][0])+len(de_en_pairs[0][1]))\n",
    "# print(len(de_en_pairs[1500][0])+len(de_en_pairs[1500][1]))\n",
    "# de_en_pairs = de_en_pairs[1500:]\n",
    "# print(len(de_en_pairs[0][0])+len(de_en_pairs[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid Dataloader  input:[S,B],mask:[B,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (/data2/zrs/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "valid_dataset = load_dataset(\"wmt14\", 'de-en', split='validation')\n",
    "\n",
    "valid_de_en_pairs = []\n",
    "for i in range(len(valid_dataset)):\n",
    "    valid_de_en_pairs.append((valid_dataset[i]['translation']['de'],valid_dataset[i]['translation']['en']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchloader input:[S,B],mask:[B,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "def batch_generator(dataset,gpu_num=1,max_len=1500):\n",
    "    en_cnt = 0\n",
    "    de_cnt = 0\n",
    "    en_batch = []\n",
    "    de_batch = []\n",
    "    batch_size = 0\n",
    "    for pairs in dataset:\n",
    "        \n",
    "        en_batch.append(pairs[1])\n",
    "        de_batch.append(pairs[0])\n",
    "        en_cnt += len(pairs[1])\n",
    "        de_cnt += len(pairs[0])\n",
    "        batch_size += 1\n",
    "        \n",
    "        if batch_size%gpu_num == 0:          \n",
    "            if en_cnt + de_cnt > max_len*gpu_num:\n",
    "\n",
    "                en_output = tokenizer.encode_batch(en_batch[:-gpu_num])\n",
    "                de_output = tokenizer.encode_batch(de_batch[:-gpu_num])\n",
    "                \n",
    "                \n",
    "                en_ids = [] \n",
    "                de_ids = []\n",
    "                target_en_ids = []\n",
    "                target_de_ids = []\n",
    "                en_padding_mask = []\n",
    "                de_padding_mask = []\n",
    "\n",
    "                for en in en_output:\n",
    "                    en_ids.append(en.ids)\n",
    "                    target_en_ids.append(en.ids[1:]+[pad_id])\n",
    "                    en_padding_mask.append(en.attention_mask)\n",
    "                    \n",
    "                for de in de_output:\n",
    "                    de_ids.append(de.ids)\n",
    "                    target_de_ids.append(de.ids[1:]+[pad_id])\n",
    "                    de_padding_mask.append(de.attention_mask)              \n",
    "\n",
    "                yield torch.LongTensor(en_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(de_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(target_en_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(target_de_ids).t().contiguous(),\\\n",
    "                        torch.BoolTensor(1-np.array(en_padding_mask)),\\\n",
    "                        torch.BoolTensor(1-np.array(de_padding_mask))\n",
    "            \n",
    "\n",
    "                en_cnt = 0\n",
    "                de_cnt = 0            \n",
    "                en_batch = en_batch[-gpu_num:]\n",
    "                de_batch = de_batch[-gpu_num:]            \n",
    "\n",
    "    if en_ids:\n",
    "        yield torch.LongTensor(en_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(de_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(target_en_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(target_de_ids).t().contiguous(),\\\n",
    "                torch.BoolTensor(1-np.array(en_padding_mask)),\\\n",
    "                torch.BoolTensor(1-np.array(de_padding_mask))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class FoodDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.data = dataset\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        example = self.data[index]\n",
    "        return example[0],example[1],example[2],example[3],example[4],example[5]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [batch for batch in batch_generator(dataset=de_en_pairs,gpu_num=gpu_num)]\n",
    "valid_list = [batch for batch in batch_generator(dataset=valid_de_en_pairs,gpu_num=gpu_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(train_list)\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "valid_dataset = FoodDataset(valid_list)\n",
    "valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch Transfomer(by Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.172152Z",
     "iopub.status.busy": "2023-02-17T12:57:43.171584Z",
     "iopub.status.idle": "2023-02-17T12:57:43.185476Z",
     "shell.execute_reply": "2023-02-17T12:57:43.183864Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.172119Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = p_drop, max_len: int = 40000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation, encoder key padding_mask and decoder key padding mask differnt with paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.188067Z",
     "iopub.status.busy": "2023-02-17T12:57:43.187619Z",
     "iopub.status.idle": "2023-02-17T12:57:43.199421Z",
     "shell.execute_reply": "2023-02-17T12:57:43.197937Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.188025Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,ntoken=n_tokens,d_model=d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.emb = nn.Embedding(ntoken,d_model,padding_idx=pad_id)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model,nhead=nhead,dim_feedforward=dff,\n",
    "                                               dropout=p_drop,activation='gelu')\n",
    "        self.encoder = TransformerEncoder(encoder_layer,N)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model,nhead=nhead,dim_feedforward=dff,\n",
    "                                               dropout=p_drop,activation='gelu')\n",
    "        self.decoder = TransformerDecoder(decoder_layer,N)\n",
    "    \n",
    "    def forward(self,src,tgt,tgt_mask,src_key_padding_mask,tgt_key_padding_mask):\n",
    "        # src:[S,B] tgt:[T,B] tgt_mask:[T,T] src_key_padding_mask:[N,S] tgt_key_padding_mask:[N,T]\n",
    "        # E=d_model\n",
    "        src_emb = self.emb(src)*math.sqrt(self.d_model)  #src:[S,B] -> src_emb:[S,B,E]\n",
    "        tgt_emb = self.emb(tgt)*math.sqrt(self.d_model)  #tgt:[T,B] -> tgt_emb:[T,B,E]\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        # emb = embedding*sqrt(d_model) + PosEmbedding : [S,B,E]\n",
    "        # tgt_mask:[T,T]\n",
    "        \n",
    "        src_hidden = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask) #[S,B,E]\n",
    "        \n",
    "        tgt_hidden = self.decoder(tgt_emb,src_hidden,tgt_mask=tgt_mask,\\\n",
    "                                  memory_key_padding_mask=src_key_padding_mask,\\\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask) #[T,B,E]\n",
    "                                 \n",
    "        \n",
    "        return F.linear(tgt_hidden,self.emb.weight) # Tying Weight [T,B,ntokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "* de->en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.202058Z",
     "iopub.status.busy": "2023-02-17T12:57:43.201558Z",
     "iopub.status.idle": "2023-02-17T12:57:43.852498Z",
     "shell.execute_reply": "2023-02-17T12:57:43.851312Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.202017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (emb): Embedding(37000, 512, padding_idx=2)\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id,label_smoothing=epsilon) # Label Smooth\n",
    "transformer_model = TransformerModel()\n",
    "transformer_model.to(device)\n",
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.854607Z",
     "iopub.status.busy": "2023-02-17T12:57:43.854166Z",
     "iopub.status.idle": "2023-02-17T12:57:43.861068Z",
     "shell.execute_reply": "2023-02-17T12:57:43.859090Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.854571Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     print(scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.863383Z",
     "iopub.status.busy": "2023-02-17T12:57:43.862992Z",
     "iopub.status.idle": "2023-02-17T12:57:43.872194Z",
     "shell.execute_reply": "2023-02-17T12:57:43.871193Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.863349Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install fvcore -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.875779Z",
     "iopub.status.busy": "2023-02-17T12:57:43.874243Z",
     "iopub.status.idle": "2023-02-17T12:57:43.886037Z",
     "shell.execute_reply": "2023-02-17T12:57:43.885175Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.875724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                | #elements or shape   |\n",
      "|:--------------------|:---------------------|\n",
      "| model               | 63.1M                |\n",
      "|  emb                |  18.9M               |\n",
      "|   emb.weight        |   (37000, 512)       |\n",
      "|  encoder            |  18.9M               |\n",
      "|   encoder.layers    |   18.9M              |\n",
      "|    encoder.layers.0 |    3.2M              |\n",
      "|    encoder.layers.1 |    3.2M              |\n",
      "|    encoder.layers.2 |    3.2M              |\n",
      "|    encoder.layers.3 |    3.2M              |\n",
      "|    encoder.layers.4 |    3.2M              |\n",
      "|    encoder.layers.5 |    3.2M              |\n",
      "|  decoder            |  25.2M               |\n",
      "|   decoder.layers    |   25.2M              |\n",
      "|    decoder.layers.0 |    4.2M              |\n",
      "|    decoder.layers.1 |    4.2M              |\n",
      "|    decoder.layers.2 |    4.2M              |\n",
      "|    decoder.layers.3 |    4.2M              |\n",
      "|    decoder.layers.4 |    4.2M              |\n",
      "|    decoder.layers.5 |    4.2M              |\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "print(parameter_count_table(transformer_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path,\n",
    "                    epoch,\n",
    "                    modules,\n",
    "                    optimizers,\n",
    "                    schedulers,\n",
    "                    step_list,\n",
    "                    train_loss_list,\n",
    "                    val_loss_list,\n",
    "                    safe_replacement: bool = True):\n",
    "\n",
    "    if isinstance(modules, torch.nn.Module):\n",
    "        modules = [modules]\n",
    "    if isinstance(optimizers, torch.optim.Optimizer):\n",
    "        optimizers = [optimizers]\n",
    "    if not isinstance(schedulers, list):\n",
    "        schedulers = [schedulers]\n",
    "    # Data dictionary to be saved\n",
    "    data = {\n",
    "        'epoch': epoch,\n",
    "        # Current time (UNIX timestamp)\n",
    "        'time': time.time(),\n",
    "        # State dict for all the modules\n",
    "        'modules': [m.state_dict() for m in modules],\n",
    "        # State dict for all the optimizers\n",
    "        'optimizers': [o.state_dict() for o in optimizers],\n",
    "        'schedulers': [s.state_dict() for s in schedulers],\n",
    "        \"step_list\":step_list,\n",
    "        \"train_loss_list\":train_loss_list,\n",
    "        \"val_loss_list\":val_loss_list\n",
    "    }\n",
    "\n",
    "    # Safe replacement of old checkpoint\n",
    "\n",
    "    if os.path.exists(path) and safe_replacement:\n",
    "        # There's an old checkpoint. Rename it!\n",
    "        temp_file = path + '.old'\n",
    "        abandon_file = path + '.abandon'\n",
    "        \n",
    "        if os.path.exists(temp_file):\n",
    "            os.rename(temp_file,abandon_file)\n",
    "        \n",
    "        os.rename(path, temp_file)\n",
    "        \n",
    "        if os.path.exists(abandon_file):\n",
    "            os.unlink(abandon_file)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Save the new checkpoint\n",
    "    with open(path, 'wb') as fp:\n",
    "        torch.save(data, fp)\n",
    "        # Flush and sync the FS\n",
    "        fp.flush()\n",
    "        os.fsync(fp.fileno())\n",
    "\n",
    "    print(\"save to \",path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def load_checkpoint(path,\n",
    "                    default_epoch,\n",
    "                    modules,\n",
    "                    optimizers,\n",
    "                    schedulers,\n",
    "                    step_list,\n",
    "                    train_loss_list,\n",
    "                    val_loss_list,\n",
    "                    verbose: bool = True):\n",
    "\n",
    "    if isinstance(modules, torch.nn.Module):\n",
    "        modules = [modules]\n",
    "    if isinstance(optimizers, torch.optim.Optimizer):\n",
    "        optimizers = [optimizers]\n",
    "    if not isinstance(schedulers, list):\n",
    "        schedulers = [schedulers]\n",
    "        \n",
    "    # If there's a checkpoint\n",
    "    if os.path.exists(path):\n",
    "        # Load data\n",
    "        data = torch.load(path, map_location=next(modules[0].parameters()).device)\n",
    "\n",
    "        # Inform the user that we are loading the checkpoint\n",
    "        if verbose:\n",
    "            print(f\"Loaded checkpoint saved at {datetime.fromtimestamp(data['time']).strftime('%Y-%m-%d %H:%M:%S')}. \"\n",
    "                  f\"Resuming from epoch {data['epoch']}\")\n",
    "\n",
    "        # Load state for all the modules\n",
    "        for i, m in enumerate(modules):\n",
    "            modules[i].load_state_dict(data['modules'][i])\n",
    "\n",
    "        # Load state for all the optimizers\n",
    "        for i, o in enumerate(optimizers):\n",
    "            optimizers[i].load_state_dict(data['optimizers'][i])\n",
    "\n",
    "        for i, s in enumerate(schedulers):\n",
    "            schedulers[i].load_state_dict(data['schedulers'][i])\n",
    "            \n",
    "        step_list.clear()\n",
    "        step_list += data['step_list']\n",
    "        \n",
    "        train_loss_list.clear()\n",
    "        train_loss_list += data['train_loss_list']        \n",
    "        \n",
    "        val_loss_list.clear()\n",
    "        val_loss_list += data['val_loss_list']\n",
    "        \n",
    "        # Next epoch\n",
    "        return data['epoch'] + 1\n",
    "    else:\n",
    "        return default_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (train de<=>en i.e intertranslation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int):\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = generate_square_subsequent_mask(38481) #38481 is the max_len  occur too much memory \n",
    "# mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.888380Z",
     "iopub.status.busy": "2023-02-17T12:57:43.887928Z",
     "iopub.status.idle": "2023-02-17T12:57:43.906996Z",
     "shell.execute_reply": "2023-02-17T12:57:43.905341Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.888328Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def train(model,epoch):\n",
    "    \n",
    "    \n",
    "    lambda1 = lambda step_num: min((step_num+1)**(-0.5),(step_num+1)*(warmup_steps**(-1.5)))\n",
    "    optimizer = torch.optim.Adam(transformer_model.parameters(),lr=d_model**(-0.5),betas=(0.9,0.98),eps=1e-9)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    loss_list = []\n",
    "    valid_list = []\n",
    "    step_list = []    \n",
    "\n",
    "    steps = 0\n",
    "    \n",
    "    load_checkpoint(path=save_path,\n",
    "                    default_epoch=epoch,\n",
    "                    modules=model,\n",
    "                    optimizers=optimizer,\n",
    "                    schedulers=scheduler,\n",
    "                    step_list=step_list,\n",
    "                    train_loss_list=loss_list,\n",
    "                    val_loss_list=valid_list)\n",
    "                   \n",
    "    if step_list:\n",
    "        steps = step_list[-1]\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    log_interval = 2000\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    for en_ids,de_ids,target_en_ids,target_de_ids,\\\n",
    "        en_padding_mask,de_padding_mask in train_loader:\n",
    "\n",
    "        en_ids = en_ids.squeeze().to(device)\n",
    "        de_ids = de_ids.squeeze().to(device)\n",
    "        target_en_ids = target_en_ids.squeeze().to(device)\n",
    "        target_de_ids = target_de_ids.squeeze().to(device)\n",
    "        en_padding_mask = en_padding_mask.squeeze().to(device)\n",
    "        de_padding_mask = de_padding_mask.squeeze().to(device)\n",
    "        \n",
    "#         print(en_ids.shape,de_ids.shape,en_padding_mask.shape)\n",
    "        # en_ids:[T,B],de_ids:[S,B],target_en_ids:[T,B],target_de_ids:[S,B]\n",
    "        # en_padding_mask:[B,T] de_padding_mask:[B,S]\n",
    "        \n",
    "        # mask_slide:[T,T]\n",
    "        #target_de_ids:$de<eos> en_ids:<bos>$en<eos>\n",
    "#         output = model(target_de_ids,en_ids,mask[:en_ids.shape[0]][:en_ids.shape[0]])\n",
    "\n",
    "\n",
    "        # de -> en\n",
    "        #de_ids:$<bos>de<eos> en_ids:<bos>$en<eos>\n",
    "        output = model(de_ids,en_ids,\\\n",
    "                       generate_square_subsequent_mask(en_ids.shape[0]).to(device),\\\n",
    "                       de_padding_mask,en_padding_mask)\n",
    "                       \n",
    "        # output:[T,B,ntokens]\n",
    "\n",
    "        loss = 0.5*criterion(output.view(-1,n_tokens),target_en_ids.view(-1))\n",
    "        \n",
    "        \n",
    "        output = model(en_ids,de_ids,\\\n",
    "                       generate_square_subsequent_mask(de_ids.shape[0]).to(device),\\\n",
    "                       en_padding_mask,de_padding_mask)\n",
    "                       \n",
    "\n",
    "        loss += 0.5*criterion(output.view(-1,n_tokens),target_de_ids.view(-1))       \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        steps += 1\n",
    "        total_loss += loss.item()\n",
    "        if steps%log_interval ==0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            s_per_step = (time.time() - start_time) / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| steps {steps:5d}|'\n",
    "                  f'lr {lr} | s/step {s_per_step:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            loss_list.append(cur_loss)\n",
    "            step_list.append(steps)\n",
    "            valid_list.append(evaluate(model,valid_loader))\n",
    "            \n",
    "            save_checkpoint(path=save_path,\n",
    "                    epoch=epoch,\n",
    "                    modules=model,\n",
    "                    optimizers=optimizer,\n",
    "                    schedulers=scheduler,\n",
    "                    step_list=step_list,\n",
    "                    train_loss_list=loss_list,\n",
    "                    val_loss_list=valid_list)\n",
    "                    \n",
    "\n",
    "    save_checkpoint(path=save_path,\n",
    "            epoch=epoch,\n",
    "            modules=model,\n",
    "            optimizers=optimizer,\n",
    "            schedulers=scheduler,\n",
    "            step_list=step_list,\n",
    "            train_loss_list=loss_list,\n",
    "            val_loss_list=valid_list)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (only test de->en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:46.461572Z",
     "iopub.status.busy": "2023-02-17T12:57:46.460368Z",
     "iopub.status.idle": "2023-02-17T12:57:46.603425Z",
     "shell.execute_reply": "2023-02-17T12:57:46.602446Z",
     "shell.execute_reply.started": "2023-02-17T12:57:46.461515Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def evaluate(model, valid_loader):\n",
    "    print('='*30)\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    cnt=0\n",
    "    pred_token_list = []\n",
    "    en_token_list = []   \n",
    "    \n",
    "    flag = 1\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for en_ids,de_ids,target_en_ids,target_de_ids,\\\n",
    "            en_padding_mask,de_padding_mask in valid_loader:\n",
    "            \n",
    "            en_ids = en_ids.squeeze().to(device)\n",
    "            de_ids = de_ids.squeeze().to(device)\n",
    "            target_en_ids = target_en_ids.squeeze().to(device)\n",
    "            target_de_ids = target_de_ids.squeeze().to(device)\n",
    "            en_padding_mask = en_padding_mask.squeeze().to(device)\n",
    "            de_padding_mask = de_padding_mask.squeeze().to(device)\n",
    "\n",
    "            \n",
    "            # en_ids:[T,B],de_ids:[S,B],target_en_ids:[T,B],target_de_ids:[S,B]\n",
    "            # en_padding_mask:[B,T] de_padding_mask:[B,S]\n",
    "\n",
    "            # mask_slide:[T,T]\n",
    "            #target_de_ids:$de<eos> en_ids:<bos>$en<eos>\n",
    "    #         output = model(target_de_ids,en_ids,mask[:en_ids.shape[0]][:en_ids.shape[0]])\n",
    "\n",
    "    \n",
    "            # de -> en\n",
    "            #de_ids:$<bos>de<eos> en_ids:<bos>$en<eos>\n",
    "            output = model(de_ids,en_ids,\\\n",
    "                           generate_square_subsequent_mask(en_ids.shape[0]).to(device),\\\n",
    "                           de_padding_mask,en_padding_mask)\n",
    "\n",
    "            # output:[T,B,ntokens]\n",
    "            # target_en_ids:[T,B]\n",
    "\n",
    "            loss = criterion(output.view(-1,n_tokens),target_en_ids.view(-1))\n",
    "        \n",
    "            \n",
    "            total_loss += en_ids.shape[1]*loss.item() #B*mean_loss\n",
    "            cnt += en_ids.shape[1]\n",
    "            \n",
    "            pred = torch.argmax(output,dim=-1)\n",
    "            # pred[T,B]  target_en_ids[T,B]  tokens_id\n",
    "            \n",
    "            pred = pred.t()\n",
    "            target_en_ids = target_en_ids.t()\n",
    "            # pred[B,T]  target_en_ids[B,T]  tokens_id            \n",
    "            \n",
    "            \n",
    "            sents = tokenizer.decode_batch(pred.tolist())\n",
    "            #[B,T(id)] ->[B(str)] \n",
    "            if flag:\n",
    "                print(sents[0])\n",
    "            \n",
    "    \n",
    "            pred_output = tokenizer.encode_batch(sents)\n",
    "            for o in pred_output:\n",
    "                #o.tokens :[T(str)]\n",
    "                \n",
    "                token_list = []\n",
    "                for token in o.tokens:\n",
    "                    if token == eos_id:\n",
    "                        break\n",
    "                    token_list.append(token)\n",
    "                pred_token_list.append(token_list)\n",
    "                # pred_token_list [allB,T(str)]\n",
    "            \n",
    "            true_sents = tokenizer.decode_batch(target_en_ids.tolist())\n",
    "            #[B,T(id)] -> [B(str)] \n",
    "            if flag:\n",
    "                print(true_sents[0])\n",
    "                flag=0\n",
    "            \n",
    "            \n",
    "            target_output = tokenizer.encode_batch(true_sents) \n",
    "            for o in target_output:\n",
    "                #o.tokens :[T(str)]\n",
    "                en_token_list.append([o.tokens])\n",
    "                # en_token_list [allB,1,T(str)]\n",
    "            \n",
    "    \n",
    "    avg_loss = total_loss/cnt\n",
    "    print(f\"valid_loss:{avg_loss:.5f}\")\n",
    "    \n",
    "#     print(len(pred_token_list),len(en_token_list))\n",
    "    bleu = bleu_score(pred_token_list,en_token_list)\n",
    "    print(f\"bleu:{bleu}\")\n",
    "    \n",
    "#     # pred_token_list [allB,T(str)] # en_token_list [allB,1,T(str)]\n",
    "#     print(pred_token_list[0][:20],en_token_list[0])\n",
    "        \n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:46.606409Z",
     "iopub.status.busy": "2023-02-17T12:57:46.604991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint saved at 2023-02-28 21:16:26. Resuming from epoch 1\n",
      "| steps  8000|lr 6.177095595460744e-05 | s/step  0.09 | loss 11.83 | ppl 137331.38\n",
      "==============================\n",
      " The.... be... und.. the......................\n",
      " A Republican strategy to counter the re-election of Obama\n",
      "valid_loss:9.68026\n",
      "bleu:0.2711612468859339\n",
      "save to  checkpoint.tar\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m         de_padding_mask \u001b[38;5;241m=\u001b[39m de_padding_mask\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#         print(en_ids.shape,de_ids.shape,en_padding_mask.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# en_ids:[T,B],de_ids:[S,B],target_en_ids:[T,B],target_de_ids:[S,B]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# en_padding_mask:[B,T] de_padding_mask:[B,S]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# de -> en\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m#de_ids:$<bos>de<eos> en_ids:<bos>$en<eos>\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mde_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43men_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mde_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43men_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# output:[T,B,ntokens]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,n_tokens),target_en_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# emb = embedding*sqrt(d_model) + PosEmbedding : [S,B,E]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# tgt_mask:[T,T]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m src_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_emb, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask) \u001b[38;5;66;03m#[S,B,E]\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m tgt_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43msrc_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#[T,B,E]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(tgt_hidden,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:248\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    245\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 248\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:452\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m--> 452\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    453\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:469\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    468\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 469\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:1003\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    992\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m    993\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1001\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4988\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4984\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   4985\u001b[0m \u001b[38;5;66;03m# compute in-projection\u001b[39;00m\n\u001b[1;32m   4986\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   4987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m-> 4988\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4990\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4746\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4744\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4745\u001b[0m             b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m-> 4746\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_q\u001b[49m\u001b[43m)\u001b[49m,) \u001b[38;5;241m+\u001b[39m linear(k, w_kv, b_kv)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4748\u001b[0m     w_q, w_k, w_v \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    train(transformer_model,epoch=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_model.load_state_dict(torch.load(\"latest.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps: 8092000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def paint():\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        \n",
    "        load_info = torch.load(save_path, map_location={'cuda:3': 'cuda:1'})\n",
    "        loss_list = load_info[\"train_loss_list\"]\n",
    "        valid_list = load_info[\"val_loss_list\"]\n",
    "        step_list = load_info[\"step_list\"]\n",
    "        steps = step_list[-1]\n",
    "\n",
    "    else :\n",
    "        return \"Not exist checkpoint\"\n",
    "    \n",
    "    print(\"total steps:\",steps)\n",
    "    \n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"steps\")\n",
    "    # plt.ylabel(\"loss\")\n",
    "    plt.plot(step_list[10:],loss_list[10:])\n",
    "    plt.plot(step_list[10:],valid_list[10:])\n",
    "    plt.legend([\"train loss\",\"valid loss\"])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "paint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (/data2/zrs/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"wmt14\", 'de-en', split='test')\n",
    "\n",
    "test_de_en_pairs = []\n",
    "for i in range(len(test_dataset)):\n",
    "    test_de_en_pairs.append((test_dataset[i]['translation']['de'],test_dataset[i]['translation']['en']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(save_path,map_location\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      9\u001b[0m     transformer_model\u001b[38;5;241m.\u001b[39mload_state_dict(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m \u001b[43mevaluate\u001b[49m(transformer_model,batch_generator(dataset\u001b[38;5;241m=\u001b[39mtest_de_en_pairs,gpu_num\u001b[38;5;241m=\u001b[39mgpu_num))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id) \n",
    "transformer_model = TransformerModel()\n",
    "transformer_model.to(device)\n",
    "if os.path.exists(save_path):\n",
    "\n",
    "    data = torch.load(save_path, map_location={'cuda:3': 'cuda:1'})\n",
    "    transformer_model.load_state_dict(data['modules'][0])\n",
    "\n",
    "# evaluate(transformer_model,batch_generator(dataset=test_de_en_pairs,gpu_num=gpu_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(src,references):\n",
    "    # src:str\n",
    "    output = tokenizer.encode(src)\n",
    "    \n",
    "    src_ids = [output.ids] #[1,S] \n",
    "    src_padding_mask = [1-np.array(output.attention_mask)] #[1,S]\n",
    "    tgt_ids = [[bos_id]] #[1,1] i.e [1,T]\n",
    "    \n",
    "    while tgt_ids[0][-1] != eos_id:\n",
    "        if len(tgt_ids[0]) > len(output.ids) +50:\n",
    "            break\n",
    "        pred= transformer_model(torch.LongTensor(src_ids).t().contiguous().to(device),\n",
    "                                torch.LongTensor(tgt_ids).t().contiguous().to(device),\n",
    "                                generate_square_subsequent_mask(len(tgt_ids[0])).to(device),\n",
    "                                torch.LongTensor(src_padding_mask).to(device),None)\n",
    "        # [T,1,E]\n",
    "        \n",
    "        next_token = pred.argmax(dim=-1)[-1]\n",
    "        #                      [T,1]\n",
    "        \n",
    "        # tgt_ids :<bos>       A         :[T]\n",
    "        # pred    :  A    <next token>   :[T]\n",
    "        \n",
    "        tgt_ids[0].append(next_token.item())\n",
    "    \n",
    "    # src_ids:[S] tgt_ids=[T]\n",
    "    tgt = tokenizer.decode(tgt_ids[0])\n",
    "#     print(\"\\nsrc:\",src)\n",
    "#     print(\"\\npred:\",tgt)\n",
    "    #  candidate [allB,T(str)] # references [allB,1,T(str)]\n",
    "    \n",
    "    output = tokenizer.encode(tgt)\n",
    "    candidate = [output.tokens]\n",
    "    bleu = bleu_score(candidate,references)*100\n",
    "#     print(f\"\\nbleu:{bleu}\")\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "total_bleu = 0.0\n",
    "for i in range(len(test_de_en_pairs)):\n",
    "#     print(\"=\"*40)\n",
    "    output = tokenizer.encode(test_de_en_pairs[i][1])\n",
    "    total_bleu += translate(test_de_en_pairs[i][0],references=[[output.tokens]])\n",
    "#     print(\"ans:\",test_de_en_pairs[i][1])\n",
    "\n",
    "# print(total_bleu/len(test_de_en_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
