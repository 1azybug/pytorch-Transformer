nohup: ignoring input
2
Found cached dataset wmt14 (/data2/zrs/.cache/huggingface/datasets/wmt14/de-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)
TransformerModel(
  (emb): Embedding(37000, 512, padding_idx=2)
  (pos_encoding): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
| name                | #elements or shape   |
|:--------------------|:---------------------|
| model               | 63.1M                |
|  emb                |  18.9M               |
|   emb.weight        |   (37000, 512)       |
|  encoder            |  18.9M               |
|   encoder.layers    |   18.9M              |
|    encoder.layers.0 |    3.2M              |
|    encoder.layers.1 |    3.2M              |
|    encoder.layers.2 |    3.2M              |
|    encoder.layers.3 |    3.2M              |
|    encoder.layers.4 |    3.2M              |
|    encoder.layers.5 |    3.2M              |
|  decoder            |  25.2M               |
|   decoder.layers    |   25.2M              |
|    decoder.layers.0 |    4.2M              |
|    decoder.layers.1 |    4.2M              |
|    decoder.layers.2 |    4.2M              |
|    decoder.layers.3 |    4.2M              |
|    decoder.layers.4 |    4.2M              |
|    decoder.layers.5 |    4.2M              |
Loaded checkpoint saved at 2023-03-15 07:00:25. Resuming from epoch 1
| steps 450000|lr 0.00013970843881447022 | s/step  0.08 | loss  6.39 | ppl   598.56
==============================
 A strategy of strategy of be the Obama-upction of Obama,,,,,,,,,,,,,, the the,,,,,,
 A Republican strategy to counter the re-election of Obama
valid_loss:5.80808
teacher forcing bleu:10.406682640314102
==============================
autoregressive bleu: 1.0055888526325583
best autoregressive bleu score: 1.0055888526325583
save to best_bleu.pt
save to  checkpoint.tar
| steps 500000|lr 0.00015073317636017444 | s/step  0.10 | loss  6.23 | ppl   510.21
==============================
 A strategy of strategy to be- Obama-inction of Obama............ Obama Obama Obama Obama Obama Obama Obama Obama Obama Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:5.62950
teacher forcing bleu:10.502298921346664
==============================
autoregressive bleu: 1.3754440773657577
best autoregressive bleu score: 1.3754440773657577
save to best_bleu.pt
save to  checkpoint.tar
| steps 550000|lr 0.00016176563431032241 | s/step  0.10 | loss  6.09 | ppl   442.80
==============================
 A strategy of strategy of be- strategy-exction of Obama’.....................
 A Republican strategy to counter the re-election of Obama
valid_loss:5.49771
teacher forcing bleu:8.694037050008774
==============================
autoregressive bleu: 1.528759720806345
best autoregressive bleu score: 1.528759720806345
save to best_bleu.pt
save to  checkpoint.tar
| steps 600000|lr 0.00017279037185602663 | s/step  0.10 | loss  5.95 | ppl   382.06
==============================
 A strategy of strategy is the- strategy-scted of Obama,. the the the the the the,,,,,,, the the the the the the the
 A Republican strategy to counter the re-election of Obama
valid_loss:5.39227
teacher forcing bleu:6.5895408391952515
==============================
autoregressive bleu: 1.5137583177929994
save to  checkpoint.tar
| steps 650000|lr 0.0001838228298061746 | s/step  0.10 | loss  5.82 | ppl   337.40
==============================
 A strategy of strategy of the- Obama-exction of Obama’.....................
 A Republican strategy to counter the re-election of Obama
valid_loss:5.24127
teacher forcing bleu:12.443894892930984
==============================
autoregressive bleu: 2.230779385466441
best autoregressive bleu score: 2.230779385466441
save to best_bleu.pt
save to  checkpoint.tar
| steps 700000|lr 0.00019485528775632259 | s/step  0.10 | loss  5.71 | ppl   301.61
==============================
 A strategy of strategy of be- strategy-excted of Obama...............
 A Republican strategy to counter the re-election of Obama
valid_loss:5.11144
teacher forcing bleu:17.923058569431305
==============================
autoregressive bleu: 2.363906224050678
best autoregressive bleu score: 2.363906224050678
save to best_bleu.pt
save to  checkpoint.tar
| steps 750000|lr 0.00020588002530202683 | s/step  0.09 | loss  5.60 | ppl   271.75
==============================
 A strategy of strategy of the- Obama-exction of Obama................. states....
 A Republican strategy to counter the re-election of Obama
valid_loss:5.04690
teacher forcing bleu:14.535236358642578
==============================
autoregressive bleu: 2.639240624486681
best autoregressive bleu score: 2.639240624486681
save to best_bleu.pt
save to  checkpoint.tar
| steps 800000|lr 0.0002169124832521748 | s/step  0.10 | loss  5.51 | ppl   246.74
==============================
 A strategy of strategy of the Obama strategy-exction of Obama policy policy policy states states states states........ states states states states.
 A Republican strategy to counter the re-election of Obama
valid_loss:4.94352
teacher forcing bleu:18.28307956457138
==============================
autoregressive bleu: 3.123176990354866
best autoregressive bleu score: 3.123176990354866
save to best_bleu.pt
save to  checkpoint.tar
| steps 850000|lr 0.000227937220797879 | s/step  0.10 | loss  5.42 | ppl   225.67
==============================
 A strategy of strategy of the the strategy-election of the ofal from of of of of of of from from of of of of of of of
 A Republican strategy to counter the re-election of Obama
valid_loss:4.87659
teacher forcing bleu:18.048281967639923
==============================
autoregressive bleu: 3.1439433921303945
best autoregressive bleu score: 3.1439433921303945
save to best_bleu.pt
save to  checkpoint.tar
| steps 900000|lr 0.000238969678748027 | s/step  0.10 | loss  5.34 | ppl   208.22
==============================
 A strategy of strategy of theact Obama-establishction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.80119
teacher forcing bleu:17.917566001415253
==============================
autoregressive bleu: 3.822926872392751
best autoregressive bleu score: 3.822926872392751
save to best_bleu.pt
save to  checkpoint.tar
| steps 950000|lr 0.0002455952094132835 | s/step  0.10 | loss  5.27 | ppl   194.13
==============================
 A strategy of strategy of be the Obama-establishction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.74175
teacher forcing bleu:15.835197269916534
==============================
autoregressive bleu: 3.6816349163771154
save to  checkpoint.tar
| steps 1000000|lr 0.0002403490634188239 | s/step  0.10 | loss  5.19 | ppl   179.07
==============================
 A strategy of strategy to be Obama Obama-establishction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.63535
teacher forcing bleu:20.12167125940323
==============================
autoregressive bleu: 4.18071824896125
best autoregressive bleu score: 4.18071824896125
save to best_bleu.pt
save to  checkpoint.tar
| steps 1050000|lr 0.00023542535376091175 | s/step  0.10 | loss  5.10 | ppl   164.56
==============================
 A strategy of strategy to be Obama Obama-establishction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.57086
teacher forcing bleu:26.25594139099121
==============================
autoregressive bleu: 4.30552290416457
best autoregressive bleu score: 4.30552290416457
save to best_bleu.pt
save to  checkpoint.tar
| steps 1100000|lr 0.0002307954965181944 | s/step  0.10 | loss  5.03 | ppl   153.55
==============================
 A strategy of strategy to be the Obama-exction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.49762
teacher forcing bleu:26.19977295398712
==============================
autoregressive bleu: 4.310802462882443
best autoregressive bleu score: 4.310802462882443
save to best_bleu.pt
save to  checkpoint.tar
| steps 1150000|lr 0.000226425497415557 | s/step  0.10 | loss  4.96 | ppl   143.17
==============================
 A strategy of strategy to be Obama Obama-establishction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.43531
teacher forcing bleu:20.966516435146332
==============================
autoregressive bleu: 4.6619505595326975
best autoregressive bleu score: 4.6619505595326975
save to best_bleu.pt
save to  checkpoint.tar
| steps 1200000|lr 0.0002222974977645892 | s/step  0.10 | loss  4.91 | ppl   135.61
==============================
 A strategy of strategy of be the Obama-usection of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.36291
teacher forcing bleu:32.0817232131958
==============================
autoregressive bleu: 5.1697151654142095
best autoregressive bleu score: 5.1697151654142095
save to best_bleu.pt
save to  checkpoint.tar
save to  checkpoint.tar
Loaded checkpoint saved at 2023-03-16 17:14:25. Resuming from epoch 1
| steps 1250000|lr 0.0002164249904426034 | s/step  0.08 | loss  4.81 | ppl   123.30
==============================
 A strategy of strategy to make Obama Obama-usection of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.29829
teacher forcing bleu:15.595591068267822
==============================
autoregressive bleu: 5.173454584641527
best autoregressive bleu score: 5.173454584641527
save to best_bleu.pt
save to  checkpoint.tar
| steps 1300000|lr 0.00021281166532236372 | s/step  0.10 | loss  4.77 | ppl   118.11
==============================
 A strategy of strategy for re Obama Obama-elected of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.25339
teacher forcing bleu:21.8205526471138
==============================
autoregressive bleu: 5.468597726652228
best autoregressive bleu score: 5.468597726652228
save to best_bleu.pt
save to  checkpoint.tar
| steps 1350000|lr 0.00020937112928702688 | s/step  0.10 | loss  4.73 | ppl   113.67
==============================
 A strategy of strategy to make Obama Obama-exction of Obama
 A Republican strategy to counter the re-election of Obama
valid_loss:4.21882
teacher forcing bleu:18.535153567790985
==============================
autoregressive bleu: 5.72969986582261
best autoregressive bleu score: 5.72969986582261
save to best_bleu.pt
save to  checkpoint.tar
