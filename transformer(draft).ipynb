{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023/2/27\n",
    "* rewrite tokenizer by hugging face\n",
    "* rewrite dataloader via yield and add key_padding_mask\n",
    "* Tying weight between embedding and pre_softmax\n",
    "* rewriting Transformer model via TransformerLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* add Label Smooth\n",
    "* rewrite train() and evaluate    almost cause by BatchLoader and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023/2/28\n",
    "* rewrite BatchLoader make the total batch_tokens <= max_len\n",
    "* merge valid_loader and train_loader to one function by argument dataset\n",
    "* writer translate function for test\n",
    "* carry BatchLoader in dataLoader of torch (by batch_size=1)\n",
    "* change de -> en to de<->en(intertranslation)\n",
    "* limited the max_len of output <= input length + 50\n",
    "* in evaluate function,delete the tokens following \\<eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023/3/14 update:\n",
    "* execute backward as soon as possible\n",
    "* checkpoint add valid BLEU score list(teacher forcing)\n",
    "* def autoregressive_evaluate method for calculating the bleu in the test environment\n",
    "* bleu*=100\n",
    "* add batch_tokens to hype-parameter\n",
    "* add gradient accumulation\n",
    "* checkpoint add valid BLEU score list(autoregressive)\n",
    "* warning fix:converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.\n",
    "* save best_bleu parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/3/25 update:\n",
    "* change de<->en to en->de(this is identical to paper) and increase the truncate_len and batch_tokens\n",
    "* for saving flops, initial the parameter from best_blue_score.pt(which train 10 epoch on de<->en i.e. parameters in Vesion3)\n",
    "* record steps//accumulation_step instead of steps(for being identical to paper's steps) \n",
    "* evaluate change de->en to en->de \n",
    "* make the calculation of eval_loss be identical to traning_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/3/27 update:\n",
    "* fixed the bug of recording step_list(bug:record the steps//accumulation and assgin steps with it)\n",
    "* change truncate_len = 768 batch_tokens = 1536\n",
    "* change accumulation_steps=1024  lr=1e-3\n",
    "* change scheduler:CosineAnnealingWarmRestarts with T_0=1024,T_mult=2,eta_min=1e-4\n",
    "* train from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/3/28 update:\n",
    "### new hype-parameter setting:\n",
    "* accumulation_steps = 256;\n",
    "* set a warmup scheduler of warmup steps =256\n",
    "* initial learning_rate = 3e-4;\n",
    "* warmup_scheduler = LinearLR(optimizer,start_factor=0.1,end_factor=1,total_iters=256)\n",
    "* CosineAnnealingWarmRestarts(optimizer, T_0=256,T_mult=2,eta_min=7e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/3/29:\n",
    "* found bug: (steps//accumulation_steps)*accumulation_steps != steps i.e. can't restore steps from step_list\n",
    "* fixed it\n",
    "* change epochs to 3 for find the hype-parameters\n",
    "* change warmup_steps from 256 to 256*32 = 8192\n",
    "* fixed bug in train():optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9,0.98),eps=1e-9) #\n",
    "* previos:optimizer = torch.optim.Adam(transformer_model.parameters(),lr=learning_rate,betas=(0.9,0.98),eps=1e-9) #\n",
    "* fixed bug about warmup_scheduler: \n",
    "* move it to next line of cosine_scheduler;otherwise lr will raise from initial lr instead of 0.1*initial lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/3/31\n",
    "* use the nearest setting with paper\n",
    "* i.e. accumulation_steps = 50000//batch_tokens=25 and scheduler in paper\n",
    "* in the sake of improving the gpu use rate, decrease truncate_len = 256 and increase batch_tokens = 2048 (more neat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023/4/1\n",
    "* overfitting\n",
    "* make truncate_len = 768 batch_tokens = 1536 accumulation_steps = 50000//batch_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future\n",
    "### Test module\n",
    "* beam search\n",
    "\n",
    "\n",
    "### Train module\n",
    "* use colossal-ai to train model (data parallel and gradient accumulation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/zrs/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed_value = 721\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value) \n",
    "torch.manual_seed(seed_value)    \n",
    "torch.cuda.manual_seed(seed_value)      \n",
    "# torch.cuda.manual_seed_all(seed_value)   \n",
    "\n",
    "torch.backends.cudnn.benchmark = False        # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/zrs/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_tokens = 37000\n",
    "bos_id = 0\n",
    "eos_id = 1\n",
    "pad_id = 2\n",
    "\n",
    "# below two hypeparameter is not need\n",
    "# seq_len = 512\n",
    "# batch_size = 8\n",
    "\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dff = 2048\n",
    "N = 6 # num of encoder/decoder layers\n",
    "p_drop = 0.1\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "gpu_num = 1\n",
    "# warmup_steps = 4000*8//gpu_num\n",
    "# start_factor = 0.1\n",
    "# end_factor = 1.0\n",
    "warmup_steps = 4000\n",
    "\n",
    "truncate_len = 768\n",
    "batch_tokens = 1536 #  the maximum of the total num of src tokens + tgt tokens\n",
    "\n",
    "accumulation_steps = (25000*2)//batch_tokens\n",
    "# learning_rate = 3e-4\n",
    "# T0 = 256\n",
    "# Tmul = 2\n",
    "# min_lr = 7e-5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "used_cuda = \"cuda:3\"\n",
    "device = torch.device(used_cuda if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "save_path = \"checkpoint.tar\"\n",
    "\n",
    "# other parameter in train() and spm.SentencePieceTrainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-17T12:09:59.376655Z",
     "iopub.status.busy": "2023-02-17T12:09:59.374968Z",
     "iopub.status.idle": "2023-02-17T12:25:08.578162Z",
     "shell.execute_reply": "2023-02-17T12:25:08.576340Z",
     "shell.execute_reply.started": "2023-02-17T12:09:59.376508Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"wmt14\", 'de-en', split='train')\n",
    "\n",
    "# with open(\"en.txt\",'w') as f:\n",
    "#     for i in range(len(dataset)):\n",
    "#         f.write(dataset[i]['translation']['en']+'\\n')\n",
    "        \n",
    "# with open(\"de.txt\",'w') as f:\n",
    "#     for i in range(len(dataset)):\n",
    "#         f.write(dataset[i]['translation']['de']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:25:08.581937Z",
     "iopub.status.busy": "2023-02-17T12:25:08.581441Z",
     "iopub.status.idle": "2023-02-17T12:25:08.588636Z",
     "shell.execute_reply": "2023-02-17T12:25:08.587034Z",
     "shell.execute_reply.started": "2023-02-17T12:25:08.581891Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncate the long sentence (though we can train transformer by any length ,the gpu memory cannot allow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:25:08.590772Z",
     "iopub.status.busy": "2023-02-17T12:25:08.590310Z",
     "iopub.status.idle": "2023-02-17T12:25:08.603615Z",
     "shell.execute_reply": "2023-02-17T12:25:08.602749Z",
     "shell.execute_reply.started": "2023-02-17T12:25:08.590725Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m de_en_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):        \n\u001b[0;32m----> 6\u001b[0m     de_en_pairs\u001b[38;5;241m.\u001b[39mappend((\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m][:truncate_len],dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m][:truncate_len]))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py:2590\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py:2575\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2573\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2574\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2575\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:634\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    632\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:406\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:441\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('dataset')\n",
    "\n",
    "de_en_pairs = []\n",
    "for i in range(len(dataset)):        \n",
    "    de_en_pairs.append((dataset[i]['translation']['de'][:truncate_len],dataset[i]['translation']['en'][:truncate_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_pairs = sorted(de_en_pairs,key=lambda x:+len(x[0])+len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(de_en_pairs[-1][0])+len(de_en_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(de_en_pairs[-500][0])+len(de_en_pairs[-500][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_en_pairs = de_en_pairs[:-500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(de_en_pairs[-1][0])+len(de_en_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(de_en_pairs[0][0])+len(de_en_pairs[0][1]))\n",
    "# print(len(de_en_pairs[1500][0])+len(de_en_pairs[1500][1]))\n",
    "# de_en_pairs = de_en_pairs[1500:]\n",
    "# print(len(de_en_pairs[0][0])+len(de_en_pairs[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid Dataloader  input:[S,B],mask:[B,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "valid_dataset = load_dataset(\"wmt14\", 'de-en', split='validation')\n",
    "\n",
    "valid_de_en_pairs = []\n",
    "for i in range(len(valid_dataset)):\n",
    "    valid_de_en_pairs.append((valid_dataset[i]['translation']['de'],valid_dataset[i]['translation']['en']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchloader input:[S,B],mask:[B,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "def batch_generator(dataset,gpu_num=1,max_len=batch_tokens):\n",
    "    en_cnt = 0\n",
    "    de_cnt = 0\n",
    "    en_batch = []\n",
    "    de_batch = []\n",
    "    batch_size = 0\n",
    "    for pairs in dataset:\n",
    "        \n",
    "        en_batch.append(pairs[1])\n",
    "        de_batch.append(pairs[0])\n",
    "        en_cnt += len(pairs[1])\n",
    "        de_cnt += len(pairs[0])\n",
    "        batch_size += 1\n",
    "        \n",
    "        if batch_size%gpu_num == 0:          \n",
    "            if en_cnt + de_cnt > max_len*gpu_num:\n",
    "\n",
    "                en_output = tokenizer.encode_batch(en_batch[:-gpu_num])\n",
    "                de_output = tokenizer.encode_batch(de_batch[:-gpu_num])\n",
    "                \n",
    "                \n",
    "                en_ids = [] \n",
    "                de_ids = []\n",
    "                target_en_ids = []\n",
    "                target_de_ids = []\n",
    "                en_padding_mask = []\n",
    "                de_padding_mask = []\n",
    "\n",
    "                for en in en_output:\n",
    "                    en_ids.append(en.ids)\n",
    "                    target_en_ids.append(en.ids[1:]+[pad_id])\n",
    "                    en_padding_mask.append(en.attention_mask)\n",
    "                    \n",
    "                for de in de_output:\n",
    "                    de_ids.append(de.ids)\n",
    "                    target_de_ids.append(de.ids[1:]+[pad_id])\n",
    "                    de_padding_mask.append(de.attention_mask)              \n",
    "\n",
    "                yield torch.LongTensor(en_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(de_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(target_en_ids).t().contiguous(),\\\n",
    "                        torch.LongTensor(target_de_ids).t().contiguous(),\\\n",
    "                        torch.BoolTensor(1-np.array(en_padding_mask)),\\\n",
    "                        torch.BoolTensor(1-np.array(de_padding_mask))\n",
    "            \n",
    "\n",
    "                en_cnt = 0\n",
    "                de_cnt = 0            \n",
    "                en_batch = en_batch[-gpu_num:]\n",
    "                de_batch = de_batch[-gpu_num:]            \n",
    "\n",
    "    if en_ids:\n",
    "        yield torch.LongTensor(en_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(de_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(target_en_ids).t().contiguous(),\\\n",
    "                torch.LongTensor(target_de_ids).t().contiguous(),\\\n",
    "                torch.BoolTensor(1-np.array(en_padding_mask)),\\\n",
    "                torch.BoolTensor(1-np.array(de_padding_mask))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class FoodDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.data = dataset\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        example = self.data[index]\n",
    "        return example[0],example[1],example[2],example[3],example[4],example[5]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [batch for batch in batch_generator(dataset=de_en_pairs,gpu_num=gpu_num)]\n",
    "valid_list = [batch for batch in batch_generator(dataset=valid_de_en_pairs,gpu_num=gpu_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(train_list)\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "valid_dataset = FoodDataset(valid_list)\n",
    "valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch Transfomer(by Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.172152Z",
     "iopub.status.busy": "2023-02-17T12:57:43.171584Z",
     "iopub.status.idle": "2023-02-17T12:57:43.185476Z",
     "shell.execute_reply": "2023-02-17T12:57:43.183864Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.172119Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = p_drop, max_len: int = 40000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation, encoder key padding_mask and decoder key padding mask differnt with paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.188067Z",
     "iopub.status.busy": "2023-02-17T12:57:43.187619Z",
     "iopub.status.idle": "2023-02-17T12:57:43.199421Z",
     "shell.execute_reply": "2023-02-17T12:57:43.197937Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.188025Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,ntoken=n_tokens,d_model=d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.emb = nn.Embedding(ntoken,d_model,padding_idx=pad_id)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model,nhead=nhead,dim_feedforward=dff,\n",
    "                                               dropout=p_drop,activation='gelu')\n",
    "        self.encoder = TransformerEncoder(encoder_layer,N)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model,nhead=nhead,dim_feedforward=dff,\n",
    "                                               dropout=p_drop,activation='gelu')\n",
    "        self.decoder = TransformerDecoder(decoder_layer,N)\n",
    "    \n",
    "    def forward(self,src,tgt,tgt_mask,src_key_padding_mask,tgt_key_padding_mask):\n",
    "        # src:[S,B] tgt:[T,B] tgt_mask:[T,T] src_key_padding_mask:[N,S] tgt_key_padding_mask:[N,T]\n",
    "        # E=d_model\n",
    "        src_emb = self.emb(src)*math.sqrt(self.d_model)  #src:[S,B] -> src_emb:[S,B,E]\n",
    "        tgt_emb = self.emb(tgt)*math.sqrt(self.d_model)  #tgt:[T,B] -> tgt_emb:[T,B,E]\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        # emb = embedding*sqrt(d_model) + PosEmbedding : [S,B,E]\n",
    "        # tgt_mask:[T,T]\n",
    "        \n",
    "        src_hidden = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask) #[S,B,E]\n",
    "        \n",
    "        tgt_hidden = self.decoder(tgt_emb,src_hidden,tgt_mask=tgt_mask,\\\n",
    "                                  memory_key_padding_mask=src_key_padding_mask,\\\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask) #[T,B,E]\n",
    "                                 \n",
    "        \n",
    "        return F.linear(tgt_hidden,self.emb.weight) # Tying Weight [T,B,ntokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "* de->en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.202058Z",
     "iopub.status.busy": "2023-02-17T12:57:43.201558Z",
     "iopub.status.idle": "2023-02-17T12:57:43.852498Z",
     "shell.execute_reply": "2023-02-17T12:57:43.851312Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.202017Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id,label_smoothing=epsilon) # Label Smooth\n",
    "transformer_model = TransformerModel()\n",
    "transformer_model.to(device)\n",
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.854607Z",
     "iopub.status.busy": "2023-02-17T12:57:43.854166Z",
     "iopub.status.idle": "2023-02-17T12:57:43.861068Z",
     "shell.execute_reply": "2023-02-17T12:57:43.859090Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.854571Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     print(scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.863383Z",
     "iopub.status.busy": "2023-02-17T12:57:43.862992Z",
     "iopub.status.idle": "2023-02-17T12:57:43.872194Z",
     "shell.execute_reply": "2023-02-17T12:57:43.871193Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.863349Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install fvcore -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.875779Z",
     "iopub.status.busy": "2023-02-17T12:57:43.874243Z",
     "iopub.status.idle": "2023-02-17T12:57:43.886037Z",
     "shell.execute_reply": "2023-02-17T12:57:43.885175Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.875724Z"
    }
   },
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "print(parameter_count_table(transformer_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load init.pt(i.e. 10epoch train on Version3 de<->en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_cuda = \"cuda:1\"\n",
    "# transformer_model.load_state_dict(torch.load('init.pt', map_location={origin_cuda: used_cuda}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path,\n",
    "                    epoch,\n",
    "                    modules,\n",
    "                    optimizers,\n",
    "                    schedulers,\n",
    "                    step_list,\n",
    "                    train_loss_list,\n",
    "                    val_loss_list,\n",
    "                    val_bleu_list,\n",
    "                    val_auto_bleu_list,\n",
    "                    safe_replacement: bool = True):\n",
    "\n",
    "    if isinstance(modules, torch.nn.Module):\n",
    "        modules = [modules]\n",
    "    if isinstance(optimizers, torch.optim.Optimizer):\n",
    "        optimizers = [optimizers]\n",
    "    if not isinstance(schedulers, list):\n",
    "        schedulers = [schedulers]\n",
    "    # Data dictionary to be saved\n",
    "    data = {\n",
    "        'epoch': epoch,\n",
    "        # Current time (UNIX timestamp)\n",
    "        'time': time.time(),\n",
    "        # State dict for all the modules\n",
    "        'modules': [m.state_dict() for m in modules],\n",
    "        # State dict for all the optimizers\n",
    "        'optimizers': [o.state_dict() for o in optimizers],\n",
    "        'schedulers': [s.state_dict() for s in schedulers],\n",
    "        \"step_list\":step_list,\n",
    "        \"train_loss_list\":train_loss_list,\n",
    "        \"val_loss_list\":val_loss_list,\n",
    "        \"val_bleu_list\":val_bleu_list,\n",
    "        \"val_auto_bleu_list\":val_auto_bleu_list\n",
    "    }\n",
    "\n",
    "    # Safe replacement of old checkpoint\n",
    "\n",
    "    if os.path.exists(path) and safe_replacement:\n",
    "        # There's an old checkpoint. Rename it!\n",
    "        temp_file = path + '.old'\n",
    "        abandon_file = path + '.abandon'\n",
    "        \n",
    "        if os.path.exists(temp_file):\n",
    "            os.rename(temp_file,abandon_file)\n",
    "        \n",
    "        os.rename(path, temp_file)\n",
    "        \n",
    "        if os.path.exists(abandon_file):\n",
    "            os.unlink(abandon_file)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Save the new checkpoint\n",
    "    with open(path, 'wb') as fp:\n",
    "        torch.save(data, fp)\n",
    "        # Flush and sync the FS\n",
    "        fp.flush()\n",
    "        os.fsync(fp.fileno())\n",
    "\n",
    "    print(\"save to \",path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def load_checkpoint(path,\n",
    "                    default_epoch,\n",
    "                    modules,\n",
    "                    optimizers,\n",
    "                    schedulers,\n",
    "                    step_list,\n",
    "                    train_loss_list,\n",
    "                    val_loss_list,\n",
    "                    val_bleu_list,\n",
    "                    val_auto_bleu_list,\n",
    "                    verbose: bool = True):\n",
    "\n",
    "    if isinstance(modules, torch.nn.Module):\n",
    "        modules = [modules]\n",
    "    if isinstance(optimizers, torch.optim.Optimizer):\n",
    "        optimizers = [optimizers]\n",
    "    if not isinstance(schedulers, list):\n",
    "        schedulers = [schedulers]\n",
    "        \n",
    "    # If there's a checkpoint\n",
    "    if os.path.exists(path):\n",
    "        # Load data\n",
    "        data = torch.load(path, map_location=next(modules[0].parameters()).device)\n",
    "\n",
    "        # Inform the user that we are loading the checkpoint\n",
    "        if verbose:\n",
    "            print(f\"Loaded checkpoint saved at {datetime.fromtimestamp(data['time']).strftime('%Y-%m-%d %H:%M:%S')}. \"\n",
    "                  f\"Resuming from epoch {data['epoch']}\")\n",
    "\n",
    "        # Load state for all the modules\n",
    "        for i, m in enumerate(modules):\n",
    "            modules[i].load_state_dict(data['modules'][i])\n",
    "\n",
    "        # Load state for all the optimizers\n",
    "        for i, o in enumerate(optimizers):\n",
    "            optimizers[i].load_state_dict(data['optimizers'][i])\n",
    "\n",
    "        for i, s in enumerate(schedulers):\n",
    "            schedulers[i].load_state_dict(data['schedulers'][i])\n",
    "            \n",
    "        step_list.clear()\n",
    "        step_list += data['step_list']\n",
    "        \n",
    "        train_loss_list.clear()\n",
    "        train_loss_list += data['train_loss_list']        \n",
    "        \n",
    "        val_loss_list.clear()\n",
    "        val_loss_list += data['val_loss_list']\n",
    "        \n",
    "        val_bleu_list.clear()\n",
    "        val_bleu_list += data['val_bleu_list']\n",
    "        \n",
    "        val_auto_bleu_list.clear()\n",
    "        val_auto_bleu_list += data['val_auto_bleu_list']\n",
    "        \n",
    "        # Next epoch\n",
    "        return data['epoch'] + 1\n",
    "    else:\n",
    "        return default_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (train de<=>en i.e intertranslation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int):\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = generate_square_subsequent_mask(38481) #38481 is the max_len  occur too much memory \n",
    "# mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:43.888380Z",
     "iopub.status.busy": "2023-02-17T12:57:43.887928Z",
     "iopub.status.idle": "2023-02-17T12:57:43.906996Z",
     "shell.execute_reply": "2023-02-17T12:57:43.905341Z",
     "shell.execute_reply.started": "2023-02-17T12:57:43.888328Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts,LinearLR\n",
    "\n",
    "def train(model,epoch):\n",
    "    \n",
    "    \n",
    "    lambda1 = lambda step_num: min((step_num+1)**(-0.5),(step_num+1)*(warmup_steps**(-1.5)))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=d_model**(-0.5),betas=(0.9,0.98),eps=1e-9)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9,0.98),eps=1e-9)\n",
    "    \n",
    "#     scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T0,T_mult=Tmul,eta_min=min_lr)\n",
    "    \n",
    "    # warmup_scheduler must be the last scheduler being define, otherwise lr won't initial by start_factor*initial_lr\n",
    "#     warmup_scheduler = LinearLR(optimizer,start_factor=start_factor,end_factor=end_factor,total_iters=warmup_steps)\n",
    "    \n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_bleu_list = []\n",
    "    val_auto_bleu_list = []\n",
    "    step_list = []    \n",
    "\n",
    "    steps = 0\n",
    "    best_bleu = 0\n",
    "    \n",
    "#     print(\"checkpoint1:steps:\",steps)\n",
    "    load_checkpoint(path=save_path,\n",
    "                    default_epoch=epoch,\n",
    "                    modules=model,\n",
    "                    optimizers=optimizer,\n",
    "                    schedulers=scheduler,\n",
    "                    step_list=step_list,\n",
    "                    train_loss_list=loss_list,\n",
    "                    val_loss_list=val_loss_list,\n",
    "                    val_bleu_list=val_bleu_list,\n",
    "                    val_auto_bleu_list=val_auto_bleu_list)\n",
    "#     print(\"checkpoint_load:steps_list:\",step_list)\n",
    "    if step_list:\n",
    "        steps = step_list[-1] \n",
    "#     print(\"checkpoint2:steps:\",steps)\n",
    "    if val_auto_bleu_list:\n",
    "        best_bleu = max(val_auto_bleu_list)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    log_interval = 50000\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for en_ids,de_ids,target_en_ids,target_de_ids,\\\n",
    "        en_padding_mask,de_padding_mask in train_loader:\n",
    "\n",
    "        en_ids = en_ids.squeeze().to(device)\n",
    "        de_ids = de_ids.squeeze().to(device)\n",
    "        target_en_ids = target_en_ids.squeeze().to(device)\n",
    "        target_de_ids = target_de_ids.squeeze().to(device)\n",
    "        en_padding_mask = en_padding_mask.squeeze().to(device)\n",
    "        de_padding_mask = de_padding_mask.squeeze().to(device)\n",
    "        \n",
    "#         print(en_ids.shape,de_ids.shape,en_padding_mask.shape)\n",
    "        # en_ids:[T,B],de_ids:[S,B],target_en_ids:[T,B],target_de_ids:[S,B]\n",
    "        # en_padding_mask:[B,T] de_padding_mask:[B,S]\n",
    "        \n",
    "        # mask_slide:[T,T]\n",
    "        #target_de_ids:$de<eos> en_ids:<bos>$en<eos>\n",
    "#         output = model(target_de_ids,en_ids,mask[:en_ids.shape[0]][:en_ids.shape[0]])\n",
    "\n",
    "\n",
    "#         # de -> en\n",
    "#         #de_ids:$<bos>de<eos> en_ids:<bos>$en<eos>\n",
    "#         output = model(de_ids,en_ids,\\\n",
    "#                        generate_square_subsequent_mask(en_ids.shape[0]).to(device),\\\n",
    "#                        de_padding_mask,en_padding_mask)\n",
    "                       \n",
    "#         # output:[T,B,ntokens]\n",
    "\n",
    "#         loss = 0.5*criterion(output.view(-1,n_tokens),target_en_ids.view(-1))\n",
    "#         total_loss += loss.item()\n",
    "#         loss = loss/accumulation_steps\n",
    "#         loss.backward()\n",
    "        \n",
    "        #en -> de\n",
    "        output = model(en_ids,de_ids,\\\n",
    "                       generate_square_subsequent_mask(de_ids.shape[0]).to(device),\\\n",
    "                       en_padding_mask,de_padding_mask)\n",
    "                       \n",
    "\n",
    "        loss = criterion(output.view(-1,n_tokens),target_de_ids.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        loss = loss/accumulation_steps        \n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        steps += 1\n",
    "        if steps%accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "#             if steps//accumulation_steps <= warmup_steps:\n",
    "#                 warmup_scheduler.step()\n",
    "#             else:\n",
    "#                 scheduler.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "\n",
    "        if steps%log_interval == 0:       \n",
    "            \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            s_per_step = (time.time() - start_time) / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| steps {steps//accumulation_steps:5d}|'\n",
    "                  f'lr {lr} | s/step {s_per_step:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            loss_list.append(cur_loss)\n",
    "            step_list.append(steps)\n",
    "#             print(\"checkpoint3:steps:\",steps)\n",
    "#             print(\"checkpoint4:steps_list:\",step_list)\n",
    "            \n",
    "            val_loss_list_e, val_bleu_list_e = evaluate(model,valid_loader)\n",
    "            val_loss_list.append(val_loss_list_e)\n",
    "            val_bleu_list.append(val_bleu_list_e)\n",
    "            \n",
    "            val_auto_bleu_list_e = autoregressive_evaluate(model,valid_de_en_pairs)\n",
    "            val_auto_bleu_list.append(val_auto_bleu_list_e)\n",
    "            \n",
    "            if val_auto_bleu_list_e>best_bleu:\n",
    "                best_bleu = val_auto_bleu_list_e\n",
    "                print(\"best autoregressive bleu score:\",best_bleu)\n",
    "                torch.save(model.state_dict(),\"best_bleu.pt\")\n",
    "                print(\"save to best_bleu.pt\")\n",
    "            \n",
    "            \n",
    "            save_checkpoint(path=save_path,\n",
    "                    epoch=epoch,\n",
    "                    modules=model,\n",
    "                    optimizers=optimizer,\n",
    "                    schedulers=scheduler,\n",
    "                    step_list=step_list,\n",
    "                    train_loss_list=loss_list,\n",
    "                    val_loss_list=val_loss_list,\n",
    "                    val_bleu_list=val_bleu_list,\n",
    "                    val_auto_bleu_list=val_auto_bleu_list)\n",
    "                        \n",
    "                    \n",
    "\n",
    "    save_checkpoint(path=save_path,\n",
    "            epoch=epoch,\n",
    "            modules=model,\n",
    "            optimizers=optimizer,\n",
    "            schedulers=scheduler,\n",
    "            step_list=step_list,\n",
    "            train_loss_list=loss_list,\n",
    "            val_loss_list=val_loss_list,\n",
    "            val_bleu_list=val_bleu_list,\n",
    "            val_auto_bleu_list=val_auto_bleu_list)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (~~only test de->en~~)(only en->de now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:46.461572Z",
     "iopub.status.busy": "2023-02-17T12:57:46.460368Z",
     "iopub.status.idle": "2023-02-17T12:57:46.603425Z",
     "shell.execute_reply": "2023-02-17T12:57:46.602446Z",
     "shell.execute_reply.started": "2023-02-17T12:57:46.461515Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def evaluate(model, valid_loader): # \n",
    "    print('='*30)\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    cnt=0\n",
    "    pred_token_list = []\n",
    "    de_token_list = []   \n",
    "    \n",
    "    flag = 1\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for en_ids,de_ids,target_en_ids,target_de_ids,\\\n",
    "            en_padding_mask,de_padding_mask in valid_loader:\n",
    "            \n",
    "            en_ids = en_ids.squeeze().to(device)\n",
    "            de_ids = de_ids.squeeze().to(device)\n",
    "            target_en_ids = target_en_ids.squeeze().to(device)\n",
    "            target_de_ids = target_de_ids.squeeze().to(device)\n",
    "            en_padding_mask = en_padding_mask.squeeze().to(device)\n",
    "            de_padding_mask = de_padding_mask.squeeze().to(device)\n",
    "\n",
    "            \n",
    "            # en_ids:[T,B],de_ids:[S,B],target_en_ids:[T,B],target_de_ids:[S,B]\n",
    "            # en_padding_mask:[B,T] de_padding_mask:[B,S]\n",
    "\n",
    "            # mask_slide:[T,T]\n",
    "            #target_de_ids:$de<eos> en_ids:<bos>$en<eos>\n",
    "    #         output = model(target_de_ids,en_ids,mask[:en_ids.shape[0]][:en_ids.shape[0]])\n",
    "\n",
    "    \n",
    "#             # de -> en\n",
    "#             #de_ids:$<bos>de<eos> en_ids:<bos>$en<eos>\n",
    "#             output = model(de_ids,en_ids,\\\n",
    "#                            generate_square_subsequent_mask(en_ids.shape[0]).to(device),\\\n",
    "#                            de_padding_mask,en_padding_mask)\n",
    "\n",
    "#             # output:[T,B,ntokens]\n",
    "#             # target_en_ids:[T,B]\n",
    "\n",
    "#             loss = criterion(output.view(-1,n_tokens),target_en_ids.view(-1))\n",
    "    \n",
    "            # EN -> DE\n",
    "\n",
    "            output = model(en_ids,de_ids,\\\n",
    "                           generate_square_subsequent_mask(de_ids.shape[0]).to(device),\\\n",
    "                           en_padding_mask,de_padding_mask)\n",
    "\n",
    "\n",
    "            loss = criterion(output.view(-1,n_tokens),target_de_ids.view(-1))\n",
    "\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            cnt += 1\n",
    "            \n",
    "            pred = torch.argmax(output,dim=-1)\n",
    "            # pred[T,B]  target_de_ids[T,B]  tokens_id\n",
    "            \n",
    "            pred = pred.t()\n",
    "            target_de_ids = target_de_ids.t()\n",
    "            # pred[B,T]  target_de_ids[B,T]  tokens_id            \n",
    "            \n",
    "            \n",
    "            sents = tokenizer.decode_batch(pred.tolist())\n",
    "            #[B,T(id)] ->[B(str)] \n",
    "            if flag:\n",
    "                print(\"eval_pred:\",sents[0])\n",
    "            \n",
    "    \n",
    "            pred_output = tokenizer.encode_batch(sents)\n",
    "            for o in pred_output:\n",
    "                #o.tokens :[T(str)]\n",
    "                \n",
    "                token_list = []\n",
    "                for token in o.tokens:\n",
    "                    if token == eos_id:\n",
    "                        break\n",
    "                    token_list.append(token)\n",
    "                pred_token_list.append(token_list)\n",
    "                # pred_token_list [allB,T(str)]\n",
    "            \n",
    "            true_sents = tokenizer.decode_batch(target_de_ids.tolist())\n",
    "            #[B,T(id)] -> [B(str)] \n",
    "            if flag:\n",
    "                print(\"eval_ans:\",true_sents[0])\n",
    "                flag=0\n",
    "            \n",
    "            \n",
    "            target_output = tokenizer.encode_batch(true_sents) \n",
    "            for o in target_output:\n",
    "                #o.tokens :[T(str)]\n",
    "                de_token_list.append([o.tokens])\n",
    "                # en_token_list [allB,1,T(str)]\n",
    "            \n",
    "    \n",
    "    avg_loss = total_loss/cnt\n",
    "    print(f\"valid_loss:{avg_loss:.5f}\")\n",
    "    \n",
    "#     print(len(pred_token_list),len(en_token_list))\n",
    "    bleu = bleu_score(pred_token_list,de_token_list)*100\n",
    "    print(f\"teacher forcing bleu:{bleu}\")\n",
    "    \n",
    "#     # pred_token_list [allB,T(str)] # en_token_list [allB,1,T(str)]\n",
    "#     print(pred_token_list[0][:20],en_token_list[0])\n",
    "        \n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoregressive translate( de -> en )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src, references):\n",
    "    # src:str\n",
    "    output = tokenizer.encode(src)\n",
    "    \n",
    "    src_ids = [output.ids] #[1,S] \n",
    "    src_padding_mask = np.array([1-np.array(output.attention_mask)]) #[1,S]\n",
    "    tgt_ids = [[bos_id]] #[1,1] i.e [1,T]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while tgt_ids[0][-1] != eos_id:\n",
    "            if len(tgt_ids[0]) > len(output.ids) +50:\n",
    "                break\n",
    "            pred = model(torch.LongTensor(src_ids).t().contiguous().to(device),\n",
    "                                    torch.LongTensor(tgt_ids).t().contiguous().to(device),\n",
    "                                    generate_square_subsequent_mask(len(tgt_ids[0])).to(device),\n",
    "                                    torch.LongTensor(src_padding_mask).to(device),None)\n",
    "            # [T,1,ntokens]\n",
    "\n",
    "            next_token = pred.argmax(dim=-1)[-1]\n",
    "            #                      [T,1]\n",
    "\n",
    "            # tgt_ids :<bos>       A         :[T]\n",
    "            # pred    :  A    <next token>   :[T]\n",
    "\n",
    "            tgt_ids[0].append(next_token.item())\n",
    "            # tgt_ids:[1,T]->[1,T+1]\n",
    "            \n",
    "    # tgt_ids:[1,T], tgt_ids[0]:[T]\n",
    "    tgt = tokenizer.decode(tgt_ids[0])\n",
    "#     print(\"\\nsrc:\",src)\n",
    "#     print(\"\\npred:\",tgt)\n",
    "    \n",
    "    output = tokenizer.encode(tgt)\n",
    "    candidate = [output.tokens] #  candidate [allB(1),T(str)] # references [allB(1),1,T(str)]\n",
    "    bleu = bleu_score(candidate,references)*100\n",
    "#     print(f\"\\nbleu:{bleu}\")\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoregressive evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def autoregressive_evaluate(model, pairs):\n",
    "    print('='*30)\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    \n",
    "\n",
    "    total_bleu = 0.0\n",
    "#     print(\"num of valid sample:\",len(pairs))\n",
    "    for i in range(len(pairs)):\n",
    "    #     print(\"=\"*40)\n",
    "#         print(i)\n",
    "        output = tokenizer.encode(pairs[i][0])\n",
    "        total_bleu += translate(model,pairs[i][1],references=[[output.tokens]])  # references [allB(1),1,T(str)]  \n",
    "    \n",
    "    avg_bleu = total_bleu/len(pairs)\n",
    "    print(\"autoregressive bleu:\",avg_bleu)\n",
    "    \n",
    "    model.train()\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T12:57:46.606409Z",
     "iopub.status.busy": "2023-02-17T12:57:46.604991Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    train(transformer_model,epoch=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
